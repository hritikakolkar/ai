### Projects
- [Recognizing Textual Entailment on Customer query and reason for the query](./recognizing_textual_entailment)
- Nerual Machine Translation using Scaled Dot-Product Attention (transformer architecture).
- Neural Machine Transltaion using Luong Attention
- Neural Machine Translation using Bahdanau Attention
- Sequence to Sequence Machine Translation using Neural Networks

### Research Papers
- Early and influential Papers on Attention
    + [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) | Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio
    + [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025) | Minh-Thang Luong, Hieu Pham, Christopher D. Manning
    + [***Attention Is All You Need***](https://arxiv.org/abs/1706.03762) | Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
    + [(ULMFiT) Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146) | Jeremy Howard, Sebastian Ruder
    + [Self-Attention with Relative Position Representations](https://arxiv.org/abs/1803.02155) | Peter Shaw, Jakob Uszkoreit, Ashish Vaswani
- Bi-Directional Encoder Representation Transformers Architecture
    - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) | Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
    - [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) | Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov
    - [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) | Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf
    - [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) | Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le
    - [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) | Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen
    - [DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing]() | Pengcheng He, Jianfeng Gao, Weizhu Chen
    - [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) | Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut
    - [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555) | Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning

- Generative Pre-Training Transformers
    + [(GPT) Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) | Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever
    + [(GPT-2) Language Models are Unsupervised Multitask Learners](https://github.com/openai/gpt-2) | Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever
    + [(GPT-3) Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) | Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, et. al
    + [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)
    + [Instruction Tuning with GPT-4](https://arxiv.org/abs/2304.03277) | Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao

- Transformers (Encoder-Decoder)
    + [(T5) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683v3) | Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu
    + [(FLAN) Fine-tuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652) | Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le
    + [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf) | Yi Tay et. al
    + [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) | Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer
    + [(XNLG) Cross-Lingual Natural Language Generation via Pre-Training](https://arxiv.org/abs/1909.10481) | Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling Mao, Heyan Huang
    + [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) | Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya

- PEFT
    + [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://arxiv.org/abs/2205.05638) | Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, Colin Raffel
    + [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) | Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen
- Language Models
    + [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) | Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou
    + [Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models](https://arxiv.org/abs/2304.09842) | Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao
    + [Hyena Hierarchy: Towards Larger Convolutional Language Models](https://arxiv.org/abs/2302.10866) | Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, Christopher Ré 
    + [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) | Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample
    + [Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning](https://arxiv.org/abs/2201.12023) | Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica
    + [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) | Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen
    + [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311) | Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, et al.
    + [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100) | Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et. al
    + [(Chinchilla) Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) | Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, et. al
    + [(Megatron NLG) Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/abs/2201.11990) | Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, et. al
    + [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446) | Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, et. al
    + [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) | Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei
    + [(PET) Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference](https://arxiv.org/pdf/2001.07676v3.pdf) | Timo Schick, Hinrich Schütze

- Surveys
    + [A Survey on Efficient Training of Transformers](https://arxiv.org/abs/2302.01107) | Bohan Zhuang, Jing Liu, Zizheng Pan, Haoyu He, Yuetian Weng, Chunhua Shen
    + [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223) | Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, et al.
    + [Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/pdf/2003.08271.pdf) | Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai & Xuanjing Huang
    + [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732) | Yi Tay, Mostafa Dehghani, Dara Bahri, Donald Metzler
    + [A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks](https://arxiv.org/abs/2306.07303) | Saidul Islam, Hanae Elmekki, Ahmed Elsebai, Jamal Bentahar, Najat Drawel, Gaith Rjoub, Witold Pedrycz

- Optimizers
    + [(LAMB) Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://arxiv.org/abs/1904.00962) | Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, Cho-Jui Hsieh
    + [(LARS) Large Batch Training of Convolutional Networks](https://arxiv.org/abs/1708.03888) | Yang You, Igor Gitman, Boris Ginsburg

- Positional Encoders
    + Absolute Positional Encoding
    + Relative Positional Encoding
        - [Self-Attention with Relative Position Representations | 2018](https://arxiv.org/abs/1803.02155) | Peter Shaw, Jakob Uszkoreit, Ashish Vaswani
        - [Position Information in Transformers: An Overview](https://direct.mit.edu/coli/article/48/3/733/111478/Position-Information-in-Transformers-An-Overview)

- Tokenizers
    + Byte Pair Encoding (SentencePiece and Tiktoken)
    + WordPiece

- Applications
    + NER
        * [UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition](https://arxiv.org/abs/2308.03279) | Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, Hoifung Poon
- Datasets
    + Natural Language Inference
        * [(SNLI) A large annotated corpus for learning natural language inference](https://arxiv.org/abs/1508.05326)
        * [(MultiNLI) A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](https://arxiv.org/abs/1704.05426)
        * [(Question NLI) GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://arxiv.org/abs/1804.07461)
        * [(RTE) The Fifth PASCAL Recognizing Textual Entailment Challenge](https://cris.fbk.eu/handle/11582/5351)
        * [SciTaiL: A Textual Entailment Dataset from Science Question Answering](https://ojs.aaai.org/index.php/AAAI/article/view/12022)
    + Question Answering
        * [RACE: Large-scale ReAding Comprehension Dataset From Examinations](https://arxiv.org/abs/1704.04683)
        * [(Story cloze) LSDSem 2017 Shared Task: The Story Cloze Test](https://aclanthology.org/W17-0906/)
    + Sentence Similarity
        * [(MSR Paraphrase Corpus) Automatically Constructing a Corpus of Sentential Paraphrases](https://aclanthology.org/I05-5002/)
        * [Quora Question Pair](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs)
        + [(STS BenchMark) SemEval-2017 Task 1: Semantic Textual Similarity - Multilingual and Cross-lingual Focused Evaluation](https://arxiv.org/abs/1708.00055)
    + Classification
        * [(Stanford Sentiment Treebank-2) Recursive deep models for semantic compositionality over a sentiment treebank.](https://aclanthology.org/D13-1170/)
        * [(CoLA) . Corpus of linguistic acceptability](https://nyu-mll.github.io/CoLA/)

### Blogs and Articles
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) | Jay Alammar
- [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) | Lilian Weng
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) | harvardnlp
- [Attention from Scratch](https://e2eml.school/transformers.html) | Brandon Rohrer
- [Attention Series by Machine Learning Matery](https://machinelearningmastery.com/category/attention/) | Jason Brownlee