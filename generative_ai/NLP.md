# NLP
+ ## LLM
    + ## Mixture of Experts
    + ## Reasoning LLMs
      + [LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://arxiv.org/abs/2502.21321)
      + [L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning](https://www.arxiv.org/abs/2503.04697)
    + ## Finetuning
+ ## LCM
    - [Large Concept Models: Language Modeling in a Sentence Representation Space](https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/)

# Blogs and Article
+ ## LLM
    + ## Mixture of Experts
      + [Mixture-of-Experts (MoE) LLMs](https://cameronrwolfe.substack.com/p/moe-llms) | Cameron R. Wolfe, Ph.D.
      + [nanoMoE: Mixture-of-Experts (MoE) LLMs from Scratch in PyTorch](https://open.substack.com/pub/cameronrwolfe/p/nano-moe) | Cameron R. Wolfe, Ph.D.
    + ## Reasoning LLMs
      + [Understanding Reasoning LLMs](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms) | Sebastian Raschka, PhD
      
    + ## Finetuning
      + [Playbook to fine-tune and deploy LLMs](https://open.substack.com/pub/decodingml/p/playbook-to-fine-tune-and-deploy) | 
