### Projects
- ***[Can Neural Networks learn standard mathematical functions like Algebraic, Trigonometric and Logarithmic Functions.](./neural_network_learning_standard_mathematical_functions.ipynb)*** Intuition behind neural network is to mimic a human brain. Well we have developed different Mathematical Functions over the course of history. An NN is afterall a composite funtion with lots of linear model stack inside one another. Here NNs learn about the function in its own manner as we have an accurate pattern in the generated synthetic data.

- ***[Deep Neural Networks for Handwritten Digit Classification](./deep_neural_networks_for_handwritten_digits_classification_with_pytorch.ipynb)*** Experimenting with the ability of neural networks to learn hidden features with a simple on well researched dataset. Neural Networks learn to do feature engineering itself.

- ***Multilayer Perceptron***

### Research Papers
- Neural Networks
    - [Learning Internal Representations by error propagation](https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf) | David E. Ruineihart, Geoffrey E. Hinton, and Ronald J. Williams
- Hyperparameter Tuning
    - [Population Based Training of Neural Networks](https://arxiv.org/abs/1711.09846) | Max Jaderberg et al.
- Optimizers
    + [An overview of gradient descent optimization algorithms](https://arxiv.org/abs/1609.04747) | Sebastian Ruder
    - First Order Methods
        + [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)
        + [(AdaGrad) Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](https://stanford.edu/~jduchi/projects/DuchiHaSi10_colt.pdf) | John Duchi, Elad Hazan, Yoram Singer
        + [AdaDelta: An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701) | Matthew D. Zeiler
        + [RMSProp: Root Mean Squared Propagation (Hinton lecture slide 6e)](http://www.cs.toronto.edu/~hinton/coursera/lecture6/lec6.pdf)
        + [(NAG) Nesterov's Accelerated Gradient and Momentum as approximations to Regularised Update Descent](https://arxiv.org/abs/1607.01981) | Aleksandar Botev, Guy Lever, David Barber
        + [On the importance of initialization and momentum in deep learning](http://proceedings.mlr.press/v28/sutskever13.pdf) | Ilya Sutskever, James Martens, George Dahl, Geoffrey Hinton
        + [(NADAM) Incorporating Nesterov Momentum into Adam](https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ) | Timothy Dozat
- Schedulers
    + [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay](https://arxiv.org/abs/1803.09820) | Leslie N. Smith
### Blogs and Articles
- BackPropagation
    + [Backpropagation paper from scratch](https://towardsdatascience.com/backpropagation-paper-from-scratch-796793789248)
- Optimizers
    + [optimization Series by Machine Learning Mastery](https://machinelearningmastery.com/category/optimization/)
    + [Gradient Descent With RMSProp from Scratch](https://machinelearningmastery.com/gradient-descent-with-rmsprop-from-scratch/)

### Courses
- [Introduction to Neural Networks and Machine Learning (University of Toronto)](https://www.cs.toronto.edu/~tijmen/csc321/lecture_notes.shtml) | Geoffrey Hinton

### Books
- [Algorithms for Optimization](https://algorithmsbook.com/optimization/files/optimization.pdf) | Mykel J. Kochenderfer, Tim A. Wheeler
